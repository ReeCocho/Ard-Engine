#version 450 core
#extension GL_EXT_scalar_block_layout : enable
#extension GL_EXT_nonuniform_qualifier : enable
#extension GL_EXT_control_flow_attributes : enable
#extension GL_EXT_debug_printf : enable

#define ARD_SET_SUN_SHAFT_INTERPOLATION 0
#define ARD_SET_CAMERA 1
#include "ard_bindings.glsl"

layout(local_size_x_id = 0) in;
layout(local_size_y_id = 1) in;
layout(local_size_z_id = 2) in;

layout(push_constant) uniform constants {
    SunShaftGenPushConstants consts;
};

#define HAS_GLOBAL_LIGHTING
#include "sun_shafts/common.glsl"

int t_to_sample_idx(float t) {
    return int(t * float(consts.sample_count_per_line));
}

vec4 get_sample_line(uint line, uint s) {
    const uint lin_idx = epipolar_line_sample(line, s);
    const uint low = epipolar_lines[lin_idx].low;
    const uint high = epipolar_lines[lin_idx].high;
    return mix(
        epipolar_lines[low].value,
        epipolar_lines[high].value,
        float(high != low) * (float(lin_idx - low) / max(float(high - low), 0.1))
    );
}

vec4 sample_line(uint line, vec2 scaled_pt, vec2 sun_uv, float t, float l, float d) {
    const int FILTER_TAPS = 2;

    float w = 0.000;
    vec3 color_accum = vec3(0);

    for (int i = -FILTER_TAPS; i <= FILTER_TAPS; i++) {
        const float filter_l = float(i) * l;
        int sample_idx = t_to_sample_idx(t + filter_l);

        if (sample_idx < -1 || sample_idx > int(consts.sample_count_per_line)) {
            continue;
        }

        sample_idx = clamp(sample_idx, 0, int(consts.sample_count_per_line - 1));

        const uint lin_idx = epipolar_line_sample(line, sample_idx);
        const uint low = epipolar_lines[lin_idx].low;
        const uint high = epipolar_lines[lin_idx].high;
        const vec4 low_value = epipolar_lines[low].value;
        const vec4 high_value = epipolar_lines[high].value;
        const vec2 sample_pos = sample_to_uv(line, sample_idx, sun_uv) * vec2(camera.aspect_ratio, 1.0);
        const vec2 pt_to_sample = sample_pos - scaled_pt;
        float weight = exp(-36.0 * dot(pt_to_sample, pt_to_sample));

        const vec4 color_depth = mix(
            low_value,
            high_value,
            float(high != low) * (float(lin_idx - low) / max(float(high - low), 0.1))
        );

        if (!isinf(d) || !isinf(color_depth.w)) {
            const float depth_diff = abs(d - color_depth.w);
            weight *= pow(clamp(consts.depth_threshold / max(depth_diff, consts.depth_threshold), 0.0, 1.0), 4.0);
        }

        color_accum += color_depth.rgb * weight;
        w += weight;
    }

    return vec4(color_accum, w);
}

vec4 sample_with_interpolation(
    uint low_line_idx, 
    uint high_line_idx, 
    float pixel_depth,
    vec2 scale_factor,
    vec2 pt,
    vec2 sun_uv
) {
    const vec2 sun_to_pt = (pt - sun_uv) * scale_factor;
    const vec2 high_uv = epipolar_line_edge_uv(high_line_idx);
    const vec2 low_uv = epipolar_line_edge_uv(low_line_idx);
    const vec2 high_uv_v = (high_uv - sun_uv) * scale_factor;
    const vec2 low_uv_v = (low_uv - sun_uv) * scale_factor;

    const float high_t = clamp(dot(sun_to_pt, high_uv_v) / dot(high_uv_v, high_uv_v), 0.0, 1.0);
    const float low_t = clamp(dot(sun_to_pt, low_uv_v) / dot(low_uv_v, low_uv_v), 0.0, 1.0);

    // NOTE: The steps we take for the bilateral filter need to be scaled by the aspect ratio of
    // the screen, since the UV space is a perfect square, but screen space may not be. We can
    // compute the scaling by taking the dot product of the direction the line is facing and the
    // horizontal (what we're doing below) and then mixing between the aspect and 1 and using it
    // as a multiplier.
    const float l = length((high_uv_v * high_t) - (low_uv_v * low_t));

    vec4 high_color = sample_line(
        high_line_idx, 
        pt * scale_factor,
        sun_uv.xy,
        high_t,
        l,
        pixel_depth
    );
    
    vec4 low_color = sample_line(
        low_line_idx,
        pt * scale_factor,
        sun_uv.xy,
        low_t, 
        l,
        pixel_depth
    );

    return vec4(
        high_color.rgb + low_color.rgb, 
        high_color.w + low_color.w
    );
}

float depth_sample(uvec2 screen_coord) {
    const vec2 depth_pt1 = vec2(screen_coord) / vec2(consts.output_dims);
    const vec2 depth_pt2 = vec2(screen_coord + uvec2(0, 1)) / vec2(consts.output_dims);
    const vec2 depth_pt3 = vec2(screen_coord + uvec2(1, 0)) / vec2(consts.output_dims);
    const vec2 depth_pt4 = vec2(screen_coord + uvec2(1, 1)) / vec2(consts.output_dims);
    const vec4 samples = camera.near_clip / vec4(
        texture(depth_tex, depth_pt1).r,
        texture(depth_tex, depth_pt2).r,
        texture(depth_tex, depth_pt3).r,
        texture(depth_tex, depth_pt4).r
    );
    // return (samples.x + samples.y + samples.z + samples.x) * 0.25;
    return max(samples.x, max(samples.y, max(samples.z, samples.w)));
}

void main() {
    // Early out if OOB
    if (gl_GlobalInvocationID.x > consts.output_dims.x || 
        gl_GlobalInvocationID.y > consts.output_dims.y
    ) {
        return;
    }

    // Determine which side of the viewing area the ray 
    // pointing from the sun to the sample intersects
    vec3 sun_uv = vec3(get_sun_uv(), 0.0);
    const vec3 pt = vec3(
        vec2(
            float(gl_GlobalInvocationID.x) / float(consts.output_dims.x),
            1.0 - (float(gl_GlobalInvocationID.y) / float(consts.output_dims.y))
        ),
        0.0
    );

    const float pixel_depth = camera.near_clip / 
        texture(
            depth_tex, 
            vec2(
                pt.x + (0.5 / float(consts.output_dims.x)), 
                (1.0 - pt.y) + (0.5 / float(consts.output_dims.y))
            )
        ).r;

    // If the sun is OOB, project it back on to the screen
    if (sun_uv.x < 0.0 || sun_uv.x > 1.0 || sun_uv.y < 0.0 || sun_uv.y > 1.0) {
        sun_uv = vec3(project_uv_to_edge(pt.xy, sun_uv.xy), 0.0);
    }

    // Project the point to the edge of the screen
    const vec2 projected_pt = project_uv_to_edge(sun_uv.xy, pt.xy);

    // TODO: This is cringe. Make it faster.
    float epipolar_line_idx_linear = 0.0;
    if (projected_pt.y == 1.0) {
        epipolar_line_idx_linear = projected_pt.x * 0.25;
    } else if (projected_pt.x == 1.0) {
        epipolar_line_idx_linear = 0.25 + ((1.0 - projected_pt.y) * 0.25);
    } else if (projected_pt.y == 0.0) {
        epipolar_line_idx_linear = 0.5 + ((1.0 - projected_pt.x) * 0.25);
    } else {
        epipolar_line_idx_linear = 0.75 + (projected_pt.y * 0.25);
    }

    const vec3 sun_to_pt = pt - sun_uv; 

    // Convert the linearized line index into the actual line indices
    const uint low_line_idx = clamp(
        uint(epipolar_line_idx_linear * float(consts.line_count)),
        0,
        consts.line_count - 1
    );
    const uint high_line_idx = (low_line_idx + 1) % consts.line_count;

    // Compute the interpolated color
    vec4 interpolated_color = sample_with_interpolation(
        low_line_idx, 
        high_line_idx,
        pixel_depth,
        vec2(camera.aspect_ratio, 1.0),
        pt.xy,
        sun_uv.xy
    );

    debugPrintfEXT("%f", interpolated_color.w);

    // If the weights are too small, we move to the next outer lines and use those instead
    if (interpolated_color.w < 0.1) {
        uint new_low = low_line_idx - 1;
        if (low_line_idx == 0) {
            new_low = consts.line_count - 1;
        }

        uint new_high = high_line_idx + 1;
        if (new_high == consts.line_count) {
            new_high = 0;
        }

        interpolated_color = sample_with_interpolation(
            new_low, 
            new_high,
            pixel_depth,
            vec2(camera.aspect_ratio, 1.0),
            pt.xy,
            sun_uv.xy
        );
    }

    imageStore(
        output_tex,
        ivec2(gl_GlobalInvocationID.xy),
        vec4(interpolated_color.w == 0.0 ? vec3(0) : interpolated_color.rgb / interpolated_color.w, 1.0)
    );
}